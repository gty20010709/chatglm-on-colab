{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX63nilmPuTK"
      },
      "source": [
        "# 如何在 Google Colab 部署 ChatGLM-6B (int4量化版本)\n",
        "\n",
        "0. 步骤\n",
        "\n",
        "  - 安装环境\n",
        "  - 下载模型\n",
        "  - 部署完成，开始使用\n",
        "\n",
        "1. 为什要安装量化的版本？\n",
        "  - Colab 免费版本所提供的运行内存只有 12G 多，不到 13G， 没有办法满足运行的需求。\n",
        "\n",
        "2. 安装有哪些注意事项？\n",
        "\n",
        "  - 注意将运行环境切换到有 GPU 的版本。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0dEhMXidXCt"
      },
      "outputs": [],
      "source": [
        "# 如果报错： NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968\n",
        "# 就把下面四行取消注释\n",
        "\n",
        "# import locale\n",
        "# def getpreferredencoding(do_setlocale = True):\n",
        "#     return \"UTF-8\"\n",
        "# locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "!pip install protobuf\n",
        "!pip install transformers==4.27.1\n",
        "!pip install cpm_kernels\n",
        "!pip install torch>=1.10\n",
        "!pip install gradio\n",
        "!pip install mdtex2html\n",
        "!pip install sentencepiece\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True).half().cuda()\n",
        "\n",
        "print(\"成功，下面是一个事例：\\n\")\n",
        "response, history = model.chat(tokenizer, \"你好\", history=[])\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GfFFihveyZg",
        "outputId": "70290a11-8b77-4cbf-db5d-a0d743d010ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure! Here is an example of how you can use Python to implement the bubble sort algorithm:\n",
            "```\n",
            "def bubble_sort(arr):\n",
            "    n = len(arr)\n",
            "    for i in range(n):\n",
            "        for j in range(i+1, n):\n",
            "            if arr[i] > arr[j]:\n",
            "                arr[i], arr[j] = arr[j], arr[i]\n",
            "    return arr\n",
            "```\n",
            "Here is how you can use this function:\n",
            "```\n",
            "arr = [9, 7, 2, 1, 8, 3, 5]\n",
            "sorted_arr = bubble_sort(arr)\n",
            "print(sorted_arr)\n",
            "```\n",
            "This will output:\n",
            "```\n",
            "[1, 2, 3, 5, 7, 9, 8]\n",
            "```\n",
            "I hope this helps! Let me know if you have any questions."
          ]
        }
      ],
      "source": [
        "history = []\n",
        "question = \"\"\"\n",
        "please use Python to write a bubble_sort\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "flag = 0\n",
        "for response, history in model.stream_chat(tokenizer, question, history=history):\n",
        "  target_response = response[flag:len(response) + 1]\n",
        "  print(target_response, end=\"\")\n",
        "  flag += len(target_response)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
